{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import urllib.request"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:52:14.684429Z",
     "start_time": "2025-06-16T06:52:14.676698Z"
    }
   },
   "id": "48ae3e2d26060e4a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset from internet...\n",
      "Preprocessing data...\n",
      "Dataset summary:\n",
      "Total reviews: 5000\n",
      "Positive reviews: 2519\n",
      "Negative reviews: 2481\n",
      "Tokenizing text...\n",
      "Vocabulary size: 54467\n",
      "Sequence length: 200\n",
      "Training samples: 4000\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB dataset from internet...\")\n",
    "url = \"https://huggingface.co/datasets/nocode-ai/imdb-movie-reviews/resolve/main/IMDB%20Dataset.csv\"\n",
    "filename = \"data/IMDB_Dataset.csv\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "df_subset = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "df_subset['review_clean'] = df_subset['review'].apply(clean_text)\n",
    "\n",
    "# Map sentiment to binary\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "df_subset['sentiment_binary'] = df_subset['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Remove any null values\n",
    "df_subset = df_subset.dropna()\n",
    "\n",
    "print(f\"Dataset summary:\")\n",
    "print(f\"Total reviews: {len(df_subset)}\")\n",
    "print(f\"Positive reviews: {sum(df_subset['sentiment_binary'])}\")\n",
    "print(f\"Negative reviews: {len(df_subset) - sum(df_subset['sentiment_binary'])}\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df_subset['review_clean'].values\n",
    "y = df_subset['sentiment_binary'].values\n",
    "\n",
    "# Tokenize text\n",
    "print(\"Tokenizing text...\")\n",
    "vocab_size = 5000\n",
    "max_length = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_length, truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Sequence length: {max_length}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:52:22.535265Z",
     "start_time": "2025-06-16T06:52:17.716008Z"
    }
   },
   "id": "c3813d9921d8e299",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "\n",
      "LSTM Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential_11\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_11 (\u001B[38;5;33mEmbedding\u001B[0m)        │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_10 (\u001B[38;5;33mLSTM\u001B[0m)                  │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_20 (\u001B[38;5;33mDense\u001B[0m)                │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (\u001B[38;5;33mDropout\u001B[0m)             │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_21 (\u001B[38;5;33mDense\u001B[0m)                │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 79ms/step - accuracy: 0.5155 - loss: 0.6928 - val_accuracy: 0.5840 - val_loss: 0.6821\n",
      "Epoch 2/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 79ms/step - accuracy: 0.6686 - loss: 0.6441 - val_accuracy: 0.7160 - val_loss: 0.5821\n",
      "Epoch 3/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 77ms/step - accuracy: 0.8057 - loss: 0.4806 - val_accuracy: 0.7130 - val_loss: 0.5725\n",
      "Epoch 4/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 77ms/step - accuracy: 0.8823 - loss: 0.3414 - val_accuracy: 0.7430 - val_loss: 0.5828\n",
      "Epoch 5/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 75ms/step - accuracy: 0.9004 - loss: 0.2818 - val_accuracy: 0.7160 - val_loss: 0.6184\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "print(\"Building LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:52:55.439519Z",
     "start_time": "2025-06-16T06:52:29.636664Z"
    }
   },
   "id": "7320dbb9707b61e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Building Simple RNN model for comparison...\n",
      "\n",
      "Simple RNN Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential_12\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_12 (\u001B[38;5;33mEmbedding\u001B[0m)        │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (\u001B[38;5;33mSimpleRNN\u001B[0m)        │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_22 (\u001B[38;5;33mDense\u001B[0m)                │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (\u001B[38;5;33mDropout\u001B[0m)            │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_23 (\u001B[38;5;33mDense\u001B[0m)                │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Simple RNN model...\n",
      "Epoch 1/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 26ms/step - accuracy: 0.5035 - loss: 0.7130 - val_accuracy: 0.5100 - val_loss: 0.6936\n",
      "Epoch 2/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 24ms/step - accuracy: 0.5093 - loss: 0.6996 - val_accuracy: 0.5340 - val_loss: 0.6913\n",
      "Epoch 3/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 24ms/step - accuracy: 0.5104 - loss: 0.7012 - val_accuracy: 0.5260 - val_loss: 0.6915\n",
      "Epoch 4/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 24ms/step - accuracy: 0.5165 - loss: 0.6984 - val_accuracy: 0.5200 - val_loss: 0.6931\n",
      "Epoch 5/5\n",
      "\u001B[1m63/63\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 25ms/step - accuracy: 0.5197 - loss: 0.6936 - val_accuracy: 0.5160 - val_loss: 0.6920\n"
     ]
    }
   ],
   "source": [
    "# Build Simple RNN model for comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Building Simple RNN model for comparison...\")\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nSimple RNN Model Architecture:\")\n",
    "rnn_model.summary()\n",
    "\n",
    "# Train RNN model\n",
    "print(\"\\nTraining Simple RNN model...\")\n",
    "rnn_history = rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:53:12.884542Z",
     "start_time": "2025-06-16T06:53:03.926646Z"
    }
   },
   "id": "f1476b5869345ffc",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "\n",
      "LSTM Model:\n",
      "  Test Accuracy: 0.7160\n",
      "  Test Loss: 0.6184\n",
      "  Final Training Accuracy: 0.8947\n",
      "  Final Validation Accuracy: 0.7160\n",
      "\n",
      "Simple RNN Model:\n",
      "  Test Accuracy: 0.5160\n",
      "  Test Loss: 0.6920\n",
      "  Final Training Accuracy: 0.5205\n",
      "  Final Validation Accuracy: 0.5160\n",
      "\n",
      "Accuracy Improvement: +38.8%\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate both models\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nLSTM Model:\")\n",
    "print(f\"  Test Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"  Test Loss: {lstm_loss:.4f}\")\n",
    "print(f\"  Final Training Accuracy: {lstm_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Accuracy: {lstm_history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nSimple RNN Model:\")\n",
    "print(f\"  Test Accuracy: {rnn_accuracy:.4f}\")\n",
    "print(f\"  Test Loss: {rnn_loss:.4f}\")\n",
    "print(f\"  Final Training Accuracy: {rnn_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Accuracy: {rnn_history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nAccuracy Improvement: {((lstm_accuracy - rnn_accuracy) / rnn_accuracy * 100):+.1f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:53:14.997005Z",
     "start_time": "2025-06-16T06:53:14.513394Z"
    }
   },
   "id": "40a8e7a972d5209f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PREDICTION COMPARISON\n",
      "==================================================\n",
      "Review                                                                           LSTM Prediction           RNN Prediction           \n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "This movie is absolutely amazing and fantastic!                                  Positive (confidence: 0.922) Positive (confidence: 0.507)\n",
      "This movie is terrible and awful                                                 Negative (confidence: 0.937) Negative (confidence: 0.509)\n",
      "The movie started very badly with poor acting but gradually improved and beca... Negative (confidence: 0.926) Positive (confidence: 0.519)\n",
      "Although the film had excellent cinematography and great visual effects, the ... Negative (confidence: 0.988) Positive (confidence: 0.516)\n",
      "Despite having a slow and boring beginning, the second half was incredible wi... Positive (confidence: 0.936) Positive (confidence: 0.509)\n",
      "The movie began with promise and good acting but became increasingly disappoi... Negative (confidence: 0.980) Positive (confidence: 0.505)\n",
      "\n",
      "Testing on actual dataset examples:\n",
      "Review                                                                           Actual     LSTM                      RNN                      \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "I really liked this Summerslam due to the look of the arena, the curtains and... Positive   Positive (confidence: 0.830) Positive (confidence: 0.521)\n",
      "Not many television shows appeal to quite as many different kinds of fans lik... Positive   Positive (confidence: 0.924) Negative (confidence: 0.516)\n",
      "The film quickly gets to a major chase scene with ever increasing destruction... Negative   Negative (confidence: 0.990) Positive (confidence: 0.521)\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "LSTM shows superior performance due to:\n",
      "  ✓ Long-term memory (avoids vanishing gradient)\n",
      "  ✓ Better handling of long sequences\n",
      "  ✓ Context understanding across sentence\n",
      "  ✓ Forget gate mechanism\n",
      "\n",
      "Simple RNN struggles with:\n",
      "  ✗ Vanishing gradient in long sequences\n",
      "  ✗ Poor long-term memory\n",
      "  ✗ Context loss in complex sentences\n",
      "\n",
      "Training completed!\n",
      "LSTM Final Accuracy: 0.8947\n",
      "RNN Final Accuracy: 0.5205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prediction functions for both models\n",
    "def predict_sentiment_lstm(text):\n",
    "    \"\"\"Predict sentiment using LSTM model\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, truncating='post')\n",
    "    prediction = lstm_model.predict(padded, verbose=0)[0][0]\n",
    "    \n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    return f\"{sentiment} (confidence: {confidence:.3f})\"\n",
    "\n",
    "def predict_sentiment_rnn(text):\n",
    "    \"\"\"Predict sentiment using Simple RNN model\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, truncating='post')\n",
    "    prediction = rnn_model.predict(padded, verbose=0)[0][0]\n",
    "    \n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    return f\"{sentiment} (confidence: {confidence:.3f})\"\n",
    "\n",
    "# Test both models on the same examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test cases that highlight LSTM's advantages (long sequences, context changes)\n",
    "comparison_cases = [\n",
    "    \"This movie is absolutely amazing and fantastic!\",\n",
    "    \"This movie is terrible and awful\",\n",
    "    \"The movie started very badly with poor acting but gradually improved and became absolutely amazing by the end\",\n",
    "    \"Although the film had excellent cinematography and great visual effects, the confusing plot and terrible acting ruined the entire experience\",\n",
    "    \"Despite having a slow and boring beginning, the second half was incredible with fantastic performances and brilliant storytelling\",\n",
    "    \"The movie began with promise and good acting but became increasingly disappointing with poor direction and awful ending\"\n",
    "]\n",
    "\n",
    "print(f\"{'Review':<80} {'LSTM Prediction':<25} {'RNN Prediction':<25}\")\n",
    "print(\"-\" * 130)\n",
    "\n",
    "for text in comparison_cases:\n",
    "    lstm_result = predict_sentiment_lstm(text)\n",
    "    rnn_result = predict_sentiment_rnn(text)\n",
    "    \n",
    "    # Truncate text for display\n",
    "    display_text = text[:77] + \"...\" if len(text) > 80 else text\n",
    "    print(f\"{display_text:<80} {lstm_result:<25} {rnn_result:<25}\")\n",
    "\n",
    "# Test on actual examples from dataset\n",
    "print(f\"\\nTesting on actual dataset examples:\")\n",
    "print(f\"{'Review':<80} {'Actual':<10} {'LSTM':<25} {'RNN':<25}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for i in range(3):\n",
    "    actual_text = df_subset.iloc[i]['review'][:77] + \"...\"\n",
    "    actual_sentiment = \"Positive\" if df_subset.iloc[i]['sentiment_binary'] == 1 else \"Negative\"\n",
    "    lstm_pred = predict_sentiment_lstm(df_subset.iloc[i]['review'])\n",
    "    rnn_pred = predict_sentiment_rnn(df_subset.iloc[i]['review'])\n",
    "    \n",
    "    print(f\"{actual_text:<80} {actual_sentiment:<10} {lstm_pred:<25} {rnn_pred:<25}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"LSTM shows superior performance due to:\")\n",
    "print(f\"  ✓ Long-term memory (avoids vanishing gradient)\")\n",
    "print(f\"  ✓ Better handling of long sequences\")\n",
    "print(f\"  ✓ Context understanding across sentence\")\n",
    "print(f\"  ✓ Forget gate mechanism\")\n",
    "print(f\"\\nSimple RNN struggles with:\")\n",
    "print(f\"  ✗ Vanishing gradient in long sequences\")\n",
    "print(f\"  ✗ Poor long-term memory\")\n",
    "print(f\"  ✗ Context loss in complex sentences\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"LSTM Final Accuracy: {lstm_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"RNN Final Accuracy: {rnn_history.history['accuracy'][-1]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-16T06:53:19.523627Z",
     "start_time": "2025-06-16T06:53:18.793508Z"
    }
   },
   "id": "5d8638153e87eb2",
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
